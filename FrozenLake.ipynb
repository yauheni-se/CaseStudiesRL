{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70ae607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:04:53.060000+02:00",
     "start_time": "2023-03-28T21:04:52.428Z"
    }
   },
   "outputs": [],
   "source": [
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ae17e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Auxilliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128ec54d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:04:53.791000+02:00",
     "start_time": "2023-03-28T21:04:52.430Z"
    },
    "code_folding": [
     93
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# actions coded as :left => 1, :down => 2, :right => 3, :up => 4\n",
    "actions = Dict(1 => [0,-1], 2 => [1,0], 3 => [0,1], 4 => [-1,0]);\n",
    "arrows = Dict(1 => '⇐', 2 => '⇓', 3 => '⇒', 4 => '⇑');\n",
    "rewards = Dict('S' => -0.05, 'G' => 1.0, 'H' => -1.0, 'F' => -0.05);\n",
    "\n",
    "grid3x3= ['S' 'F' 'F';\n",
    "          'F' 'F' 'H';\n",
    "          'F' 'F' 'G'];\n",
    "\n",
    "grid4x4= ['S' 'F' 'F' 'F';\n",
    "          'F' 'H' 'F' 'H';\n",
    "          'F' 'F' 'F' 'H';\n",
    "          'H' 'F' 'F' 'G'];\n",
    "\n",
    "function get_grid(dim, p_holes, seed = 234)\n",
    "    Random.seed!(seed)\n",
    "    grid = [rand() < p_holes ? 'H' : 'F' for i in 1:dim, j in 1:dim]\n",
    "    grid[1,1] = 'S'\n",
    "    grid[end,end] = 'G'\n",
    "    return grid\n",
    "end;\n",
    "\n",
    "function transition_matrix(grid, actions = actions)\n",
    "    T = zeros(length(grid),length(actions),length(grid))\n",
    "    i2s = CartesianIndices(grid)\n",
    "    s2i = LinearIndices(grid)\n",
    "    for i = 1:length(grid) # \n",
    "        if !(grid[i] == 'H' || grid[i] == 'G')\n",
    "            index = i2s[i]\n",
    "            for j = 1:length(actions)\n",
    "                indices = Tuple(index) .+ actions[j]\n",
    "                if all(in.( indices, (1:size(grid,1), 1:size(grid,2)))) \n",
    "                    k = s2i[indices...]\n",
    "                    T[k,j,i] += 0.8\n",
    "                    if actions[j][1] == 0\n",
    "                        for l in [-1,1]\n",
    "                            ind = Tuple(index) .+ (l,0)\n",
    "                            if all(in.( ind, (1:size(grid,1), 1:size(grid,2)))) \n",
    "                                T[s2i[ind...],j,i] += 0.1\n",
    "                            else\n",
    "                                T[i,j,i] += 0.1\n",
    "                            end\n",
    "                        end\n",
    "                    else\n",
    "                        for l in [-1,1]\n",
    "                            ind = Tuple(index) .+ (0,l)\n",
    "                            if all(in.( ind, (1:size(grid,1), 1:size(grid,2))))\n",
    "                                T[s2i[ind...],j,i] += 0.1\n",
    "                            else\n",
    "                                T[i,j,i] += 0.1\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                else\n",
    "                    T[i,j,i] += 0.8\n",
    "                    if actions[j][1] == 0\n",
    "                        for l in [-1,1]\n",
    "                            ind = Tuple(index) .+ (l,0)\n",
    "                            if all(in.( ind, (1:size(grid,1), 1:size(grid,2)))) \n",
    "                                T[s2i[ind...],j,i] += 0.1\n",
    "                            else\n",
    "                                T[i,j,i] += 0.1\n",
    "                            end\n",
    "                        end\n",
    "                    else\n",
    "                        for l in [-1,1]\n",
    "                            ind = Tuple(index) .+ (0,l)\n",
    "                            if all(in.( ind, (1:size(grid,1), 1:size(grid,2)))) \n",
    "                                T[s2i[ind...],j,i] += 0.1\n",
    "                            else\n",
    "                                T[i,j,i] += 0.1\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return T\n",
    "end;\n",
    "\n",
    "function reward_matrix(grid, rewards = rewards)\n",
    "    R = zeros(size(grid))\n",
    "    for i = 1:length(grid)\n",
    "        R[i] = rewards[grid[i]]\n",
    "    end\n",
    "    return R\n",
    "end;\n",
    "\n",
    "\n",
    "random_policy(grid,actions = actions) = rand(1:length(actions),size(grid));\n",
    "\n",
    "\n",
    "function print_policy(P, grid, arrows = arrows)\n",
    "    Policy = rand(Char,size(grid))\n",
    "    for i = 1:length(grid)\n",
    "        if grid[i] == 'F' || grid[i] == 'S' \n",
    "            Policy[i] = arrows[P[i]]\n",
    "        elseif grid[i] == 'H' \n",
    "            Policy[i] = '⦷'\n",
    "        else\n",
    "            Policy[i] = grid[i]\n",
    "        end\n",
    "    end\n",
    "    return Policy\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557263f5",
   "metadata": {},
   "source": [
    "### Policy Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba534bd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:04:53.796000+02:00",
     "start_time": "2023-03-28T21:04:52.431Z"
    }
   },
   "outputs": [],
   "source": [
    "# nagroda dla stanu s w iteracji i = \n",
    "# nagroda pierwotna dla stanu s + \n",
    "# beta * (srednia nagroda ze wszystkich stanów, \n",
    "#         do których możemy przejść ze stanu s, wzięta z poprzeniej iteracji i-1)\n",
    "function evaluate!(P, v, v₁, R, T, β)\n",
    "    for s = 1:length(v)\n",
    "        v₁[s]= R[s] + β * sum(v .*  T[:,P[s],s])\n",
    "    end\n",
    "end;\n",
    "\n",
    "function evaluate_policy(grid,P; β = 0.999, ϵ=0.0001, actions = actions)\n",
    "    iter = 0\n",
    "    T = transition_matrix(grid)\n",
    "    R = reward_matrix(grid)\n",
    "    v₁ = zeros(length(grid))\n",
    "    while true\n",
    "        iter += 1\n",
    "        v = deepcopy(v₁)\n",
    "        evaluate!(P, v, v₁, R, T, β)\n",
    "        #@info v₁\n",
    "        δ = maximum(abs.(v₁ - v)) \n",
    "        δ < ϵ * (1 - β) / β && break \n",
    "    end\n",
    "    return v₁\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111385fe",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Policy Iteration Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b4b84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:04:53.800000+02:00",
     "start_time": "2023-03-28T21:04:52.432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function improve_policy!(v, T, P, actions = actions)\n",
    "    for s = 1:length(v)\n",
    "        actions_vector = zeros(length(actions))\n",
    "        for a = 1:length(actions)\n",
    "            actions_vector[a] = sum(v .* T[:,a,s])\n",
    "        end\n",
    "        action = argmax(actions_vector)\n",
    "        action != P[s] && (P[s] = action)\n",
    "    end\n",
    "end;\n",
    "\n",
    "function policy_iteration(grid,β = 0.999, ϵ=0.0001)\n",
    "    iter = 1\n",
    "    T = transition_matrix(grid)\n",
    "    R = reward_matrix(grid)\n",
    "    v₁ = zeros(length(grid))\n",
    "    P = random_policy(grid)\n",
    "    while true\n",
    "        iter += 1\n",
    "        v = deepcopy(v₁)\n",
    "        evaluate!(P, v, v₁, R, T, β)\n",
    "        δ = maximum( abs.(v₁ - v)) \n",
    "        δ < ϵ * (1 - β) / β && break \n",
    "        improve_policy!(v₁, T, P)\n",
    "    end \n",
    "    println(\"Iterations: $(iter)\")\n",
    "    return reshape(v₁,size(grid)),  print_policy(P, grid)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be390a94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:04:54.916000+02:00",
     "start_time": "2023-03-28T21:04:52.433Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 45\n"
     ]
    }
   ],
   "source": [
    "vₚ, pₚ = policy_iteration(grid3x3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f6545a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:04:56.360000+02:00",
     "start_time": "2023-03-28T21:04:52.434Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 0.700269  0.646294   0.407308\n",
       " 0.770455  0.721464  -1.0\n",
       " 0.840105  0.912426   1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vₚ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8962a271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:04:56.745000+02:00",
     "start_time": "2023-03-28T21:04:52.435Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Char}:\n",
       " '⇓'  '⇐'  '⇐'\n",
       " '⇓'  '⇐'  '⦷'\n",
       " '⇒'  '⇒'  'G'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pₚ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae3add",
   "metadata": {},
   "source": [
    "### Brute force algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5874611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:04:56.759000+02:00",
     "start_time": "2023-03-28T21:04:52.436Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_sel = deepcopy(grid3x3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3067502a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:21:49.699000+02:00",
     "start_time": "2023-03-28T21:04:52.437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interations: 25000\n",
      "Interations: 50000\n",
      "Interations: 75000\n",
      "Interations: 100000\n",
      "Interations: 125000\n",
      "Interations: 150000\n",
      "Interations: 175000\n",
      "Interations: 200000\n",
      "Interations: 225000\n",
      "Interations: 250000\n"
     ]
    }
   ],
   "source": [
    "grid_length = length(grid_sel);    # liczba komórek w siatcy\n",
    "\n",
    "values = [1, 2, 3, 4];\n",
    "iter = 0;\n",
    "\n",
    "best_policy = ones(Int, grid_length);\n",
    "best_policy_val = ones(Int, grid_length);\n",
    "best_policy_sum_val = 0\n",
    "\n",
    "for i in values, j in values, k in values, l in values, m in values, \n",
    "    n in values, o in values, p in values, q in values   # tutaj jak było by 4x4 to bylo by 16 pętli\n",
    "    \n",
    "    iter += 1\n",
    "    \n",
    "    if divrem(iter, 25000)[2] == 0\n",
    "        println(\"Interations: $(iter)\")\n",
    "    end\n",
    "    \n",
    "    policy = [i, j, k, l, m, n, o, p, q]             # zbiór kierunków ruchu\n",
    "    policy_val = evaluate_policy(grid_sel, policy)   # ewaluacja całej strategii\n",
    "    policy_sum_val = sum(policy_val)                 # wartość całkowita rozważanej strategii,\n",
    "    \n",
    "    if policy_sum_val > best_policy_sum_val          # którą zmierzamy zmaksymalizować\n",
    "        best_policy = policy\n",
    "        best_policy_val = policy_val\n",
    "        best_policy_sum_val = policy_sum_val\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d151c7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T23:21:49.714000+02:00",
     "start_time": "2023-03-28T21:04:52.438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 262144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Char}:\n",
       " '⇓'  '⇐'  '⇐'\n",
       " '⇓'  '⇐'  '⦷'\n",
       " '⇒'  '⇒'  'G'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_policy_brute = reshape(best_policy, size(grid_sel));\n",
    "println(\"Iterations: $(iter)\")\n",
    "print_policy(best_policy_brute, grid_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a09dc35",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39700c66",
   "metadata": {},
   "source": [
    "W zadaniu domowym zostały porównane 2 metody do rozwiązania gry Frozen Lake: podejścia programowania dynamicznego pokazane na zajęciach oraz algorytm wyczerpujący, zaproponowany przeze mnie. Dla przykładu została rozważona gra o wymiarach 3 na 3 (o przyczynie tego później).\n",
    "\n",
    "W przypadku programowania dynamicznego dla macierzy gry 3 × 3 algorytm zbiegł do rozwiązania optymalnego za 45 iteracji (1.32 sekund na mojej maszynie, AMD Ryzen 7 4800H Radeon Graphics 2.90 GHz, 32 RAM, Windows 10 x64).\n",
    "\n",
    "Natomiast w przypadku algorytmu brute force, rozwiązanie najlepsze zostało znaleziono za (262 144) iteracji (14 m 47 sekund na mojej maszynie). \n",
    "\n",
    "Z tego wynika oczywista decyzja na rozpatrzenie mniejszej siatki gry. W tym problemie algorytm brute force ma złożoność obliczeniową O(4^n²) — rozważamy tyłe unikalnych kombinacji komórek siatki. W przypadku siatki 3x3 jest to liczba 4⁽³ˣ³⁾ = 4⁹ = (262 144). W przypadku siatek 4 × 4 i 8 × 8 wymagane będzie (4 294 967 296) i 3.402+38 liczby iteracji odpowiednio. \n",
    "\n",
    "Możemy też obliczyć czas trwania 1 iteracji. Skoro (262 144) iteracji były osiągnięte za 14 m 47 sec (887 sec), długość trwania jednej iteracji jest równa 0.003 sec. Oznacza to, że zakładając ten sam czas dla 1 iteracji, rozwiązanie dla siatki 4 × 4 byłoby znaleziono za 0.003x4294967296=1.45e7 sec (115 dni). Dla siatki 8 × 8 jest to liczba 3.17e+28 lat! Dla porównania Wszechświat istnieje 13.787e9 lat! (13.787 billionów)\n",
    "\n",
    "Dlatego konieczne jest zastosowanie innych metod poszukiwania rozwiązania problemu. Na szczęście, grę Frozen Lake można rozbić na podproblemy powtarzające i skorzystać z zasady optymalności Belmana. \n",
    "\n",
    "Zauważymy też, że otrzymaliśmy te same rozwiązania zarówno w podejściu optymizacji dynamicznej, jak i przypadku exhaustive search, co jest zgodne z założeniem teoretycznym o gwarantowanej zbieżności rozwiązania otrzymanego za pomocą optymalizacji dynamicznej. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
